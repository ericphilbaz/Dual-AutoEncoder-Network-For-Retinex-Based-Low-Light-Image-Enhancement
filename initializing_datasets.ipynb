{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhpq5YiAYvUm"
   },
   "source": [
    "**Initializing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-mq39kMYt22"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import walk\n",
    "import numpy as np\n",
    "from random import uniform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from img_helper import rgb_to_hsv_images, image_illumination_C, noise_generator\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "path_of_dataset = \"C:/Users/thaku/OneDrive/Desktop/2d Final Project Up to date/ExDark/\"\n",
    "dataset_npy_path = \"Dataset npys/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1vwQg5NZk2z"
   },
   "source": [
    "**Creating Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "na24zK9fZBXR"
   },
   "outputs": [],
   "source": [
    "# Image parameters\n",
    "image_width = 100\n",
    "image_height = image_width\n",
    "no_of_channels = 3\n",
    "\n",
    "# Load in all images from the specified folder\n",
    "def dataset_creation(image_folder, tes_split, vd_split):\n",
    "\n",
    "    image_files=[]\n",
    "    for (dir_path, dir_names, file_names) in walk(image_folder):\n",
    "        for fname in file_names:\n",
    "            if fname is not \".ipynb_checkpoints\":\n",
    "                image_files.append(os.path.join(dir_path,fname))\n",
    "    \n",
    "    image_array = np.ndarray(shape=(len(image_files), image_width, image_height, no_of_channels),\n",
    "                         dtype=np.float32)\n",
    "\n",
    "    i = 0\n",
    "    for f in image_files:\n",
    "    \timage = load_img(f, target_size=(image_width, image_height))\n",
    "    \tvar = img_to_array(image)\n",
    "    \timage_array[i] = var\n",
    "    \ti += 1\n",
    "    \n",
    "    # Rescale the pixel values to range [0,1]\n",
    "    image_array = image_array/np.max(image_array)\n",
    "\n",
    "    # Split dataset into: 99% training and 1% test\n",
    "    training,testing = train_test_split(image_array, test_size=tes_split, random_state=13)\n",
    "\n",
    "    # Split training set into: 80% training and 20% validation\n",
    "    training,validation = train_test_split(training, test_size=vd_split, random_state=13)\n",
    "\n",
    "    return training,testing,validation\n",
    "\n",
    "# Create image datasets\n",
    "training_RGB_Low,testing_RGB_Low,validation_RGB_Low = dataset_creation(path_of_dataset, 0.01, 0.2)\n",
    "\n",
    "np.save(os.path.join(dataset_npy_path,\"training_RGB_Low.npy\"),training_RGB_Low)\n",
    "np.save(os.path.join(dataset_npy_path,\"testing_RGB_Low.npy\"),testing_RGB_Low)\n",
    "np.save(os.path.join(dataset_npy_path,\"validation_RGB_Low.npy\"),validation_RGB_Low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WreL2fU5ZpYJ"
   },
   "source": [
    "**Modifying datasets according to network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tl2C_5ylZxUa"
   },
   "outputs": [],
   "source": [
    "from img_helper import rgb_to_hsv_images, image_illumination_C, noise_generator\n",
    "# Modify datasets for networks\n",
    "training_HSV_Low = rgb_to_hsv_images(training_RGB_Low)\n",
    "validation_HSV_Low = rgb_to_hsv_images(validation_RGB_Low)\n",
    "\n",
    "training_HSV_High = image_illumination_C(training_HSV_Low, image_width)\n",
    "validation_HSV_High = image_illumination_C(validation_HSV_Low, image_width)\n",
    "\n",
    "training_HSV_High_noisy = noise_generator(training_HSV_High, image_width)\n",
    "validation_HSV_High_noisy = noise_generator(validation_HSV_High, image_width)\n",
    "\n",
    "testing_HSV_Low = rgb_to_hsv_images(testing_RGB_Low)\n",
    "\n",
    "np.save(os.path.join(dataset_npy_path,\"training_HSV_Low.npy\"),training_HSV_Low)\n",
    "np.save(os.path.join(dataset_npy_path,\"validation_HSV_Low.npy\"),validation_HSV_Low)\n",
    "\n",
    "np.save(os.path.join(dataset_npy_path,\"training_HSV_High.npy\"),training_HSV_High)\n",
    "np.save(os.path.join(dataset_npy_path,\"validation_HSV_High.npy\"),validation_HSV_High)\n",
    "\n",
    "np.save(os.path.join(dataset_npy_path,\"training_HSV_High_noisy.npy\"),training_HSV_High_noisy)\n",
    "np.save(os.path.join(dataset_npy_path,\"validation_HSV_High_noisy.npy\"),validation_HSV_High_noisy)\n",
    "\n",
    "np.save(os.path.join(dataset_npy_path,\"testing_HSV_Low.npy\"),testing_HSV_Low)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPeJD7hFSxZVb5rgOZ+qvat",
   "collapsed_sections": [],
   "name": "initializing_datasets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
